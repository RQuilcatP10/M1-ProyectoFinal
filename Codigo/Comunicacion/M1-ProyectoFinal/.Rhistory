shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(tm)
tweets <- read.csv("www/datos/Csv/vacuna_dataset_preprocesado.csv", sep=",")
table(tweets$Label)
install.packages("rpart")
install.packages("rpart.plot")
library(tm)
tweets <- read.csv("www/datos/Csv/vacuna_dataset_preprocesado.csv", sep=",")
#tenemos 1412 tweets positivos y 629 tweets negativos
table(tweets$Label)
#Creamos un corpus a partir del texto preprocesado
corpus = Corpus(VectorSource(tweets$Clean.Tweet))
length(corpus)
content(corpus[[20]])
frequencies <- DocumentTermMatrix(corpus)
#Tiene 2041 tweets (filas)
#Tiene 6976 terminos (columnas)
#la palabra mas larga tiene 31 caracteres
frequencies
inspect(frequencies[800:805, 505:515])
findFreqTerms(frequencies, lowfreq = 50)
#Ahora solo trabajaremos con los terminos que mas se repiten
#Con esta funcion sacamos lo mas usado
sparse <- removeSparseTerms(frequencies, 0.995)
sparse
#Creamos un dataframe para trabajar con SVM
tweetsSparse <- as.data.frame(as.matrix(sparse))
#Asignamos a nuestro dataframe una columna con cada palabra
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
#Asignamos el valor de la clasificacion del tweet en una nueva columna
tweetsSparse$sentiment <- tweets$Label
library(caret)
library(caTools)
library(e1071)
library(plyr)
#Seteamos el valor raiz en R para valores replicables
set.seed(12)
#Crearemos a partir de nuestro dataset, data para entrenamiento y data para test
#Asignamos un 80% de la data para entrenamiento y 20% para test
split <- sample.split(tweetsSparse$sentiment, SplitRatio =  0.8)
trainSparse = subset(tweetsSparse, split == TRUE)
testSparse = subset(tweetsSparse, split == FALSE)
library(rpart)
library(rpart.plot)
tweet.CART <- rpart(as.factor(sentiment)~., trainSparse, method='class')
prp(tweet.CART)
predictCART <- predict(tweet.CART, testSparse, type='class')
testSparse
testSparse$Label
testSparse$sentiment
table(predictCART, testSparse$sentiment)
postResample(predictCART, testSparse$sentiment)
control <- trainControl(method='cv', number=10)
metric <- 'Accuracy'
set.seed(101)
tweet.cart <- train(as.factor(sentiment)~., data=trainSparse, method='rpart',
trControl=control, metric=metric)
tweet.pred <- predict(tweet.cart, testSparse)
confusionMatrix(tweet.pred, testSparse$sentiment)
table(tweets
table(tweets)
table(tweets)
table(tweets)
tweets
tweets$Negative <- as.factor(tweets$Avg <= -1)
# Look at the number and proportion of negative tweets
table(tweets$Negative)
table(tweets$Negative)
tweets <- read.csv("www/datos/Csv/vacuna_dataset_preprocesado.csv", sep=",")
heads(tweets,10)
tweets
tweets$Label
tweets$Negative <- as.factor(tweets$Label = 0)
tweets$Negative <- as.factor(tweets$Label == 0)
table(tweets$Negative)
tweet_corpus <- Corpus(VectorSource(tweets$Tweet))
tweet_corpus
clean_corpus <- function(corp){
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeWords, stopwords('es'))
corp <- tm_map(corp, stemDocument)
}
clean_corp <- clean_corpus(tweet_corpus)
clean_corp
clean_corp[[1]][1]
frequencies <- TermDocumentMatrix(clean_corp)
findFreqTerms(frequencies, lowfreq = 20)
clean_corpus <- function(corp){
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeWords, stopwords('en'))
corp <- tm_map(corp, stemDocument)
}
clean_corp <- clean_corpus(tweet_corpus)
frequencies <- TermDocumentMatrix(clean_corp)
findFreqTerms(frequencies, lowfreq = 20)
dtm <- DocumentTermMatrix(clean_corp)
sparseData <- removeSparseTerms(dtm, sparse=0.995)
sparsedf <- as.data.frame(as.matrix(sparseData))
colnames(sparsedf) <- make.names(colnames(sparsedf))
sparsedf$Negative <- tweets$Negative
set.seed(101)
split <- sample.split(sparsedf$Negative, SplitRatio = 0.7)
trainSparse <- subset(sparsedf, split==TRUE)
testSparse <- subset(sparsedf, split==FALSE)
tweet.CART <- rpart(Negative~., trainSparse, method='class')
prp(tweet.CART)
library(caret)
predictCART <- predict(tweet.CART, testSparse, type='class')
# Compute Accuracy
table(predictCART, testSparse$Negative)
postResample(predictCART, testSparse$Negative)
control <- trainControl(method='cv', number=10)
metric <- 'Accuracy'
# CART
set.seed(101)
tweet.cart <- train(Negative~., data=trainSparse, method='rpart',
trControl=control, metric=metric)
tweet.pred <- predict(tweet.cart, testSparse)
# Create Confusion Matrix
confusionMatrix(tweet.pred, testSparse$Negative)
runApp()
plot(cars)
plot(cars)
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
library(stats)
library(shiny)
library(plotly)
library(shinythemes)
library(markdown)
library(tm)
library(caret)
source("www/Introduccion.R")
source("www/Recoleccion.R")
source("www/datos/Modelos/CART.R", local = T)
source("www/datos/Modelos/SVM.R", local = T)
source("www/datos/Modelos/NaiveBayes.R", local = T)
source("www/datos/Modelos/KMeans.R", local = T)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
setwd("~/ProyectosR/M1-ProyectoFinal/www/datos/Imagenes")
setwd("~/ProyectosR/M1-ProyectoFinal")
runApp()
runApp()
runApp()
runApp()
